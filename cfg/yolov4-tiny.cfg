[net]
# Testing
batch=1
subdivisions=1
# Training
#batch=64
#subdivisions=16
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.1
exposure = 1.1
hue=.1
flip=0
max_chart_loss=5
letter_box=1

learning_rate=0.00261
burn_in=1000
max_batches = 112000
policy=steps
steps=78400,89600,100800
scales=.1,.1,.1
#label_smooth_eps=0.1

[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky
stopbackward=800

##################################

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=183
activation=linear



[yolo]
mask = 3,4,5
anchors = 24,44,  50,140,  166,65,  59,244,  196,150, 289,133
classes=56
num=6
#jitter=.3
jitter=0
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
#max=200
#counters_per_class=86, 271, 76, 231, 90, 139, 100, 296, 261, 83, 57, 10, 78, 409, 35, 219, 245, 17, 8, 98, 286, 174, 151, 183, 212, 26, 201, 72, 74, 2, 238, 46, 120, 222, 22, 175, 20, 236, 149, 128, 219, 6, 241, 104, 11, 172, 236, 239, 274, 312, 198, 92, 98, 204, 393, 83

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 23

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=183
activation=linear

[yolo]
mask = 0,1,2
anchors = 24,44,  50,140,  166,65,  59,244,  196,150, 289,133
classes=56
num=6
#jitter=.3
jitter=0
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
#max=200
#counters_per_class=86, 271, 76, 231, 90, 139, 100, 296, 261, 83, 57, 10, 78, 409, 35, 219, 245, 17, 8, 98, 286, 174, 151, 183, 212, 26, 201, 72, 74, 2, 238, 46, 120, 222, 22, 175, 20, 236, 149, 128, 219, 6, 241, 104, 11, 172, 236, 239, 274, 312, 198, 92, 98, 204, 393, 83
